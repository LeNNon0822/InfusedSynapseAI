const __vite__mapDeps=(i,m=__vite__mapDeps,d=(m.f||(m.f=["static/js/Homepage-bZx92kxl.js","static/js/gsap-j3sIeV-s.js","static/js/footer-DtWPrPWY.js","static/js/vue-router-DG7GcQKA.js","static/js/@vue-CtX6qY5s.js","static/js/vue-i18n-Dc8cNLb_.js","static/js/@intlify-BGsZdTW6.js","static/css/footer-Bx0fLF1u.css","static/js/vue3-video-play-BTt3U0ws.js","static/css/vue3-video-play-BwUlgh1t.css","static/js/lenis-CEOfCm6j.js","static/css/Homepage-BfmSMjT8.css","static/js/refresh-CtDX1xBW.js","static/js/careersPage-CI2A63e5.js","static/css/careersPage-D4GtniDB.css","static/js/Error404-QVQEzsqo.js","static/css/Error404-DQsfRD2Z.css"])))=>i.map(i=>d[i]);
import{d as e,q as n,f as t,O as o,t as i,P as r}from"./@vue-CtX6qY5s.js";import{c as a,a as l}from"./vue-router-DG7GcQKA.js";import{c as s}from"./vue-i18n-Dc8cNLb_.js";import{b as c}from"./vue3-video-play-BTt3U0ws.js";import{L as d}from"./lenis-CEOfCm6j.js";import{S as m,g as p}from"./gsap-j3sIeV-s.js";import"./@intlify-BGsZdTW6.js";!function(){const e=document.createElement("link").relList;if(!(e&&e.supports&&e.supports("modulepreload"))){for(const e of document.querySelectorAll('link[rel="modulepreload"]'))n(e);new MutationObserver(e=>{for(const t of e)if("childList"===t.type)for(const e of t.addedNodes)"LINK"===e.tagName&&"modulepreload"===e.rel&&n(e)}).observe(document,{childList:!0,subtree:!0})}function n(e){if(e.ep)return;e.ep=!0;const n=function(e){const n={};return e.integrity&&(n.integrity=e.integrity),e.referrerPolicy&&(n.referrerPolicy=e.referrerPolicy),"use-credentials"===e.crossOrigin?n.credentials="include":"anonymous"===e.crossOrigin?n.credentials="omit":n.credentials="same-origin",n}(e);fetch(e.href,n)}}();const g={},u=function(e,n,t){let o=Promise.resolve();if(n&&n.length>0){document.getElementsByTagName("link");const e=document.querySelector("meta[property=csp-nonce]"),t=(null==e?void 0:e.nonce)||(null==e?void 0:e.getAttribute("nonce"));o=Promise.allSettled(n.map(e=>{if((e=function(e){return"/InfusedSynapseAI/"+e}(e))in g)return;g[e]=!0;const n=e.endsWith(".css"),o=n?'[rel="stylesheet"]':"";if(document.querySelector(`link[href="${e}"]${o}`))return;const i=document.createElement("link");return i.rel=n?"stylesheet":"modulepreload",n||(i.as="script"),i.crossOrigin="",i.href=e,t&&i.setAttribute("nonce",t),document.head.appendChild(i),n?new Promise((n,t)=>{i.addEventListener("load",n),i.addEventListener("error",()=>t(new Error(`Unable to preload CSS for ${e}`)))}):void 0}))}function i(e){const n=new Event("vite:preloadError",{cancelable:!0});if(n.payload=e,window.dispatchEvent(n),!n.defaultPrevented)throw e}return o.then(n=>{for(const e of n||[])"rejected"===e.status&&i(e.reason);return e().catch(i)})},b=[{path:"/",name:"Homepage",meta:{title:"Homepage"},component:()=>u(()=>import("./Homepage-bZx92kxl.js"),__vite__mapDeps([0,1,2,3,4,5,6,7,8,9,10,11]))},{path:"/refresh",name:"Refresh",component:()=>u(()=>import("./refresh-CtDX1xBW.js"),__vite__mapDeps([12,4,3,5,6,8,9,10,1]))},{path:"/careers",name:"Careers",meta:{title:"Careers"},component:()=>u(()=>import("./careersPage-CI2A63e5.js"),__vite__mapDeps([13,2,3,4,5,6,1,7,8,9,10,14]))},{path:"/:cathAll(.*)",name:"404NotFound",meta:{title:"404 Not Found"},component:()=>u(()=>import("./Error404-QVQEzsqo.js"),__vite__mapDeps([15,4,3,5,6,8,9,10,1,16]))}],_=a({history:l("/InfusedSynapseAI/"),routes:b});_.beforeEach((e,n,t)=>{document.body.scrollTop=0,document.documentElement.scrollTop=0,t()});const h={en:{navigator:{blogs:"BLOGS",careers:"CAREERS",contactUs:"CONTACT US"},slogan:{sloganLine1:"Building Universal",sloganLine2:"Robot Intelligence"},subSlogan:"for autonomous decision-making and learning",mission:{title:"Our Mission",contentLine1:"Empower Industries with",contentLine2:"Intelligent Robotic Systems that Enhance",target1:"Efficiency",target2:"Safety",target3:"Human-robot Collaboration",and:"&",comma:","},insight:{title:"Insight is key",content:"We are developing robots that gain insight into the logic of the world through experiential learning, adapting their approaches to new tasks based on prior encounters and continuously refining their cognitive abilities."},urc:{titleLine1:"Universal Robotics",titleLine2:"Core (ABrain)",content1:"ABrain is a transformative robotics core, engineered with a universal compatibility layer that seamlessly adapts to various hardware and tasks. This robust foundation enhances cognitive functions, enabling robots to learn, reason, and respond intelligently across diverse environments.",content2:"Contact ",content3:" to explore how our innovative robotics solutions can bring adaptive, intelligent automation to your operations."},learnMore:"Learn More",aa:{titleLine1:"ABrain Adaptive",titleLine2:"Robotics Platform",content1:"Ready to pioneer next-level AI solutions for robotics? Our ABrain-powered mobile platform lets you develop adaptive applications effortlessly, with robust manipulation capabilities and easy integration—making advanced robotics as straightforward as a software deployment.",content2:"Apply for early access to explore impactful innovations in mobile robotics!"},arp:{titleLine1:"Adaptive Robotics",titleLine2:"Platform for",titleLine3:"Security & Inspection",content1:"Seeking advanced automation for inspections, data collection, or patrols? Our ABrain platform integrates intelligent sensing and decision-making to support autonomous security and inspection in complex environments, ensuring precision and adaptability.",content2:"Contact ",content3:" to learn how we can enhance your operations with intelligent robotics solutions."},vision:{title:"Our Vision",content:"To create general-purpose artificial intelligence rooted in real-world understanding and interaction."},stragegy:{title1:"Our Strategic",title2:"Funding Allies",content:"Driven by a shared vision, our mission is strengthened by the invaluable support of our dedicated funding allies."},contact:{title:"Contact Us",address:"Address: ",addressContent:"xx Provience xx City xx Street xx Building",tel:"Tel:",telNum:"xxxx-xxxxxxxxx",email:"E-Mail: ",emailAddr:"xxxxxxxxxx{'@'}mail.com"},homepage:"Homepage",career:"Careers",job:{jobTitle1:"Robotics Software Engineer Program",blockTitle1_1:"Job Responsibilities:",blockContent1_1_1:"Develop and implement software solutions for robotic systems. You will contribute to building systems that encompass navigation, planning and control, SLAM, manipulation, and/or perception. Proficiency in both general-purpose and specialized robotics applications is required.",blockContent1_1_2:"- Work closely with AI engineers to integrate state-of-the-art AI models into our robots.",blockContent1_1_3:"- Design, implement, and test software to ensure outstanding performance in navigation, planning and control, SLAM, perception, manipulation, and/or advanced behaviors.",blockContent1_1_4:"- Write and maintain production-grade C++ and Python code for robotic platforms.",blockContent1_1_5:"- Collaborate with deployment and testing engineers to ensure stable deployment and monitoring of robotic solutions across different sites, achieving excellent performance and high reliability.",blockContent1_1_6:"- Continuously improve and optimize the performance, reliability, and scalability of robotic software.",blockTitle1_2:"Job Requirements:",blockContent1_2_1:"- Bachelor's, Master's, or higher degree in Computer Science, Robotics, Engineering, or a related field, or equivalent practical experience.",blockContent1_2_2:"- Proficiency in C++ or Python programming languages.",blockContent1_2_3:"- Experience in developing and deploying software on real robots using NVIDIA development boards such as Orin or similar ARM architecture-based boards.",blockContent1_2_4:"- In-depth system-level understanding of various software modules and interfaces in robotics applications, with solid technical experience in at least one of the following areas: navigation, motion planning and control, SLAM, perception/computer vision, or manipulation.",blockContent1_2_5:"- Experience in ROS development.",blockContent1_2_6:"- Solid understanding and practical experience with software engineering principles, including algorithms, data structures, and system design.",blockContent1_2_7:"- Familiarity with integrating and deploying machine learning models in robotic systems.",blockContent1_2_8:"- Understanding of communication mechanisms such as socket and CAN, with relevant development experience.",blockContent1_2_9:"- Basic familiarity with or experience using Docker.",blockContent1_2_10:"- Proficiency with open-source project platforms such as GitHub or Gitee, with the ability to deploy and run open-source projects.",jobTitle2:"Embodied Intelligence Algorithm Engineer",blockTitle2_1:"Job Responsibilities:",blockContent2_1_1:"- Responsible for research and development of embodied intelligence-related models, such as large language models, vision-language models, and agent models.",blockContent2_1_2:"- Conduct research on robotic algorithms including reinforcement learning, imitation learning, and meta-learning.",blockContent2_1_3:"- Develop algorithms for robotic object manipulation, mobile grasping, mobile navigation, and 3D object or scene modeling in virtual/real environments, and achieve deployment testing for sim-to-real scenarios.",blockTitle2_2:"Job Requirements:",blockContent2_2_1:"- Bachelor's, Master's, or higher degree in Electronic and Information Systems, Computer Science, Communications, Automatic Control, or related fields.",blockContent2_2_2:"- Solid foundation in computer science and software engineering, with clear thinking, strong logic, and clean coding practices; proficient in C/C++ and Python; familiar with deep learning frameworks like PyTorch.",blockContent2_2_3:"- Experience with Linux, Docker, or ROS; experience with robotic virtual environments (e.g., HomeRobot, BEHAVIOR-1K, IsaacSim) is a plus.",blockContent2_2_4:"- Extensive experience in model training and debugging in fields like deep learning, computer vision, and NLP; familiarity with the latest Transformer, diffusion, and related model architectures.",blockContent2_2_5:"- Self-motivated, willing to challenge oneself, and committed to continuous learning and personal growth; aspiring to become a technical leader (promotion opportunities and competitive salary will be provided).",jobTitle3:"Internship Program: Research and Internship Projects (Student Program)",blockTitle3_1:"Job Responsibilities:",blockContent3_1_1:"- Conduct research on embodied intelligence-related models, including large language models, vision-language models, and agent models.",blockContent3_1_2:"- Develop algorithms in reinforcement learning, imitation learning, meta-learning, self-supervised learning, and brain-inspired navigation.",blockContent3_1_3:"- Track the latest advancements in embodied intelligence, apply algorithms, and study cutting-edge research papers.",blockTitle3_2:"Job Requirements:",blockContent3_2_1:"- Enrolled in relevant programs such as Computer Science, Data Science, Communication Engineering, Automation, or Neuroscience/Brain Science, with solid programming skills and some experience with deep learning models.",blockContent3_2_2:"- Familiarity with Linux, Docker, and deep learning frameworks like PyTorch. Preference will be given to candidates with experience in courses or research projects related to computer vision, deep learning, and artificial intelligence.",blockContent3_2_3:"- Passionate about computer vision, robotics, embodied intelligence, and related research, with a strong interest in conducting innovative studies.",blockTitle3_3:"What the Project Team Offers:",blockContent3_3_1:"- Opportunities to publish in top-tier conferences, including but not limited to CVPR, ICCV, ECCV, NeurIPS, and ICLR.",blockContent3_3_2:"- Comprehensive research training and a broad academic perspective.",blockContent3_3_3:"- International collaboration and personal development, with recommendations for internships, exchange visits, or advanced degree applications at leading domestic and international research institutions in computer vision and deep learning.",blockContent3_3_4:"- Access to rich computational resources, including extensive A100 GPU resources and multiple robotic hardware platforms.",blockContent3_3_5:"- Scholarships or compensation incentives for outstanding students."},displayCage:{title:"Display Cage Title"}},zh:{navigator:{blogs:"博客",careers:"职位招聘",contactUs:"联系我们"},slogan:{sloganLine1:"构建通用",sloganLine2:"机器人智能"},subSlogan:"实现自主决策与学习",mission:{title:"公司的任务",contentLine1:"通过智能机器人系统",contentLine2:"赋能各行业从而提升",target1:"效率",target2:"安全性",target3:"人机协作",and:"&",comma:","},insight:{title:"洞悉是关键",content:"我们致力于开发通过经验学习洞悉世界逻辑的机器人，使其能够基于过往经验调整任务处理方式，并持续优化自身的认知能力。"},urc:{titleLine1:"通用机器人",titleLine2:"核心 (ABrain)",content1:"ABrain 是一个变革性的机器人核心，内置通用适配层，可无缝适应不同硬件和任务需求。该稳健的核心增强了认知功能，使机器人能够学习、推理，并在多种环境中智能响应。",content2:"请联系",content3:", 了解如何通过我们的创新机器人解决方案，为您的运营带来自适应的智能自动化。"},learnMore:"了解更多",aa:{titleLine1:"ABrain 适应性",titleLine2:"机器人平台",content1:"准备好为机器人开发下一代 AI 解决方案了吗？由 ABrain 驱动的移动平台提供强大的操作能力和便捷的集成方式，使高级机器人开发像软件部署一样简单。",content2:"申请早期访问，探索移动机器人领域的创新可能性！"},arp:{titleLine1:"自适应",titleLine2:"安防与检查",titleLine3:"机器人平台",content1:"需要更智能的视觉检查、数据收集或巡逻自动化方案吗？ABrain 平台结合智能感知与决策功能，支持复杂环境中的自主安防与检查任务，确保精确与适应性。",content2:"请联系 ",content3:", 了解我们如何通过智能机器人解决方案提升您的业务。"},vision:{title:"我们的愿景",content:"构建具备真实世界理解与交互能力的通用人工智能。"},stragegy:{title1:"我们的",title2:"战略融资伙伴",content:"在共同愿景的引领下，我们的使命因敬业的融资伙伴支持而更加强大。"},contact:{title:"联系我们",address:"地址: ",addressContent:"广东省深圳市xx街道xx大厦xx号",tel:"联系电话: ",telNum:"xxxx-xxxxxxxxx",email:"电子邮件: ",emailAddr:"xxxxxxxxxx{'@'}mail.com"},homepage:"主页",career:"职业招聘",job:{jobTitle1:"机器人软件工程师",blockTitle1_1:"岗位职责:",blockContent1_1_1:"负责开发和实施机器人系统的软件解决方案。你将参与构建涵盖导航、规划与控制、SLAM、操作和/或感知的系统，需要熟练掌握通用及专用的机器人应用, 此岗位要求:",blockContent1_1_2:"- 与AI工程师紧密合作，将最先进的AI模型集成到我们的机器人中。",blockContent1_1_3:"- 设计、实施并测试软件，确保机器人在导航、规划与控制、SLAM、感知、操作和/或高级行为等方面表现卓越。",blockContent1_1_4:"- 编写并维护用于机器人平台的生产级C++和Python代码。",blockContent1_1_5:"- 与部署和测试工程师合作，确保机器人解决方案在各个站点的稳定部署和监控，实现卓越的性能与高可靠性。",blockContent1_1_6:"- 持续改进与优化机器人软件的性能、可靠性和可扩展性。",blockTitle1_2:"招聘要求:",blockContent1_2_1:"- 计算机科学、机器人学、工程学或相关领域的学士、硕士或更高学位，或具备同等实践经验。",blockContent1_2_2:"- 精通C++或Python编程语言。",blockContent1_2_3:"- 具有基于orin等英伟达开发板卡或类似的arm架构开发板卡的真实机器人上开发和部署软件的经验。",blockContent1_2_4:"- 对机器人应用中的各类软件模块及其接口有深入的系统级理解，并至少在以下一个领域具备扎实的技术经验：导航、运动规划与控制、SLAM、感知/计算机视觉或操作。",blockContent1_2_5:"- 具备ROS开发经验。",blockContent1_2_6:"- 深入理解并具备实践软件工程原理的经验，包括算法、数据结构和系统设计。",blockContent1_2_7:"- 熟悉机器人系统中的机器学习集成与部署。",blockContent1_2_8:"- 了解socket、can通信等通信机制开发流程，以及具有相关的开发经验。",blockContent1_2_9:"- 简单了解或使用过docker。",blockContent1_2_10:"- 熟练使用github、gitee等开源项目网站，可以部署并运行开源项目。",jobTitle2:"具身智能工程师",blockTitle2_1:"岗位职责:",blockContent2_1_1:"- 负责大语言模型、视觉-语言模型、agent模型等具身智能相关模型研究和开发；",blockContent2_1_2:"- 负责强化学习、模仿学习以及元学习等机器人算法研究；",blockContent2_1_3:"- 负责机器人在虚拟/真实环境中的物体操纵、移动抓取、移动导航等机器人算法, 三维物体或场景建模等算法研究, 实现sim-real的部署测试;",blockTitle2_2:"招聘要求:",blockContent2_2_1:"- 电子与信息系统、计算机、通信、自动控制及相关专业;",blockContent2_2_2:"- 有扎实的计算机科学技术与软件工程理论基础，思维清晰，逻辑严密，代码规范；熟练掌握C/C++、Python, 掌握深度学习框架PyTorch;",blockContent2_2_3:"- 熟悉Linux、Docker或ROS, 如有机器人虚拟环境(Homerobot, BEHAVIOR-1K, IssacSim等) 使用经验优先;",blockContent2_2_4:"- 有丰富的深度学习、机器视觉、NLP等领域的相关模型训练和调试经验；熟悉最新的Transformer、diffusion等模型架构;",blockContent2_2_5:"- 敢于挑战自我，力求自我突破，不断去学习吸纳新的知识；有志于成为技术领袖(给予相应的晋升通道和薪资鼓励)。",jobTitle3:"访问和实习项目（学生项目）",blockTitle3_1:"岗位职责:",blockContent3_1_1:"- 负责大语言模型、视觉-语言模型、agent模型等具身智能相关模型研究；",blockContent3_1_2:"- 负责强化学习、模仿学习、元学习、自监督学习、类脑导航等算法研究；",blockContent3_1_3:"- 负责具身智能论文和前沿技术的跟踪和算法应用；",blockTitle3_2:"学生项目要求:",blockContent3_2_1:"- 要求就读于计算机科学、大数据科学、通信工程、自动化、神经/脑科学等相关专业，具有扎实的编程能力和一定的深度学习模型经验;",blockContent3_2_2:"- 熟悉Linux、Docker和深度学习框架PyTorch, 有计算机视觉、深度学习、人工智能等课程或者科研项目经历者优先;",blockContent3_2_3:"- 对计算机视觉、机器人、具身智能及其相关研究具有极大热忱, 希望进行创新性研究;",blockTitle3_3:"项目组可提供：",blockContent3_3_1:"- 发表顶级论文的机会：包括但不限于CVPR，ICCV，ECCV，NeurIPS，ICLR等。",blockContent3_3_2:"- 系统的科研训练与全面的科研视野。",blockContent3_3_3:"- 国际化合作与个人提升：可以推荐至国内外多个顶尖的计算机视觉、深度学习研究机构、院所进行实习、交流访问和帮助硕博士申请。",blockContent3_3_4:"- 丰富的计算资源：我们拥有大量A100资源，同时我们拥有多个机器人硬件平台。",blockContent3_3_5:"- 优秀的学生也将获得奖学金/薪资鼓励。"},displayCage:{title:"展示柜标题"}}},f=(navigator.language||"en").toLocaleLowerCase(),C=s({locale:localStorage.getItem("lang")||f.split("-")[0]||"zh",fallbackLocale:"en",messages:h,legacy:!1}),k=(e,n)=>{const t=e.__vccOpts||e;for(const[o,i]of n)t[o]=i;return t},v=r(k(e({__name:"App",setup(e){const r=new d({lerp:.1,smoothWheel:!0,duration:1.2,easing:e=>Math.min(1,1.001-Math.pow(2,-10*e))});return r.on("scroll",()=>m.update),p.ticker.add(e=>{r.raf(1e3*e)}),p.ticker.lagSmoothing(0),(e,r)=>{const a=o("RouterView");return i(),n("div",null,[t(a,{class:"routerViewContainer"})])}}}),[["__scopeId","data-v-74779e48"]]));v.use(C),v.use(_),v.use(c),v.mount("#app");export{k as _};
